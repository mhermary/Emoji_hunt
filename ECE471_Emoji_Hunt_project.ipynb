{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrXtkatKfw26"
      },
      "source": [
        "# ECE471 Emoji Hunt project\n",
        "\n",
        "Our only test we are using to measure score is the SIFT_test. \n",
        "\n",
        "The others are kept just to show the evolution and process of our project. The run <...> tests are for debugging and can show emoji detection on the original image. They run quickly except for the Harris-Stephens corner detector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q8-KEgwTBKe",
        "outputId": "ece972a2-efb9-4a2f-ec00-d598ad181287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current project version:\n",
            "emojihunt==1.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install emojihunt --upgrade --q\n",
        "print(\"Current project version:\")\n",
        "!pip freeze | grep emojihunt\n",
        "from emojihunt import *\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp8vu0b4kGw3"
      },
      "source": [
        "# Template For EmojiHunt Project!\n",
        "\n",
        "## EmojiHunt()\n",
        "This is your emojihunt environment class you will use it for generating problem images, testing your configuration and getting your final scores on the trials.\n",
        "### EmojiHunt.get_config()\n",
        "This returns the dictionary of the configuration for the EmojiHunt. You can change the booleans in this dictionary to make the problem harder or easier.\n",
        "### EmojiHunt.update_config(dict)\n",
        "This writes the passed dictionary as the new configuration for the EmojiHunt. It should be formated as recived from .get_config(). \n",
        "### EmojiHunt.generate_image_and_points()\n",
        "This returns 3 objects the target image, the example emoji and the ground truth real points. This function is avalible so you can test your method without needing to perform full official tests. The emoji image and the test target image are simple cv2 formated color images (BGR, numpy uint8). The ground truth points are a list of tuples of points [(x_0,y_0)..(x_n,y_n)]\n",
        "### EmojiHunt.generate_image_and_points(ground_truth, predicted)\n",
        "This function gives the score between two sets of emoji points, the ground truth given by .generate_image_and_points() and the prediction points. \n",
        "### EmojiHunt.offical_test(function, config)\n",
        "For this function you explicitly give a callable object (either function or class method) that takes in the positional arguments of image, sample_emoji. This function then runs multiple tests on this function and prints a report based on the config and the socres. Official tests are seeded so the test for each config is always the same. \n",
        "### Examples of the usage of all these methods are given below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QK5aJpgMISE",
        "outputId": "7b6c8ea5-914b-4345-cab9-46cc0219e2d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading assets...\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169001437/169001437 [==============================] - 3s 0us/step\n"
          ]
        }
      ],
      "source": [
        "#@title Update Config\n",
        "emoji_hunt_object = EmojiHunt() \n",
        "#print(json.dumps(emoji_hunt_object.get_config(), indent=2))\n",
        "def make_cfg_easy():\n",
        "  cfg = {\n",
        "    \"emoji_transforms\": {\n",
        "      \"Add\": False,\n",
        "      \"Multiply\": False,\n",
        "      \"Cutout\": False,\n",
        "      \"CoarseDropout\": False,\n",
        "      \"CoarseSaltAndPepper\": False,\n",
        "      \"JpegCompression\": False,\n",
        "      \"BlendAlpha\": False,\n",
        "      \"BlendAlphaRegularGrid\": False,\n",
        "      \"GaussianBlur\": False,\n",
        "      \"MotionBlur\": False,\n",
        "      \"MultiplyHueAndSaturation\": False,\n",
        "      \"Grayscale\": False,\n",
        "      \"ChangeColorTemperature\": False,\n",
        "      \"SigmoidContrast\": False,\n",
        "      \"CLAHE\": False,\n",
        "      \"Emboss\": False,\n",
        "      \"DirectedEdgeDetect\": False,\n",
        "      \"Fliplr\": False,\n",
        "      \"PiecewiseAffine\": False,\n",
        "      \"PerspectiveTransform\": False,\n",
        "      \"WithPolarWarping\": False,\n",
        "      \"Rot90\": False,\n",
        "      \"ElasticTransformation\": False,\n",
        "      \"Jigsaw\": False\n",
        "    }\n",
        "  }\n",
        "  emoji_hunt_object.update_config(cfg)\n",
        "\n",
        "def make_cfg_med():\n",
        "  cfg = {\n",
        "    \"emoji_transforms\": {\n",
        "      \"Add\": True,\n",
        "      \"Multiply\": True,\n",
        "      \"Cutout\": True,\n",
        "      \"CoarseDropout\": True,\n",
        "      \"CoarseSaltAndPepper\": True,\n",
        "      \"JpegCompression\": True,\n",
        "      \"BlendAlpha\": True,\n",
        "      \"BlendAlphaRegularGrid\": True,\n",
        "      \"GaussianBlur\": True,\n",
        "      \"MotionBlur\": True,\n",
        "      \"MultiplyHueAndSaturation\": True,\n",
        "      \"Grayscale\": True,\n",
        "      \"ChangeColorTemperature\": False,\n",
        "      \"SigmoidContrast\": False,\n",
        "      \"CLAHE\": False,\n",
        "      \"Emboss\": False,\n",
        "      \"DirectedEdgeDetect\": False,\n",
        "      \"Fliplr\": False,\n",
        "      \"PiecewiseAffine\": False,\n",
        "      \"PerspectiveTransform\": False,\n",
        "      \"WithPolarWarping\": False,\n",
        "      \"Rot90\": False,\n",
        "      \"ElasticTransformation\": False,\n",
        "      \"Jigsaw\": False\n",
        "    }\n",
        "  }\n",
        "  emoji_hunt_object.update_config(cfg)\n",
        "\n",
        "def make_cfg_med2():\n",
        "  cfg = {\n",
        "    \"emoji_transforms\": {\n",
        "      \"Add\": False,\n",
        "      \"Multiply\": False,\n",
        "      \"Cutout\": False,\n",
        "      \"CoarseDropout\": False,\n",
        "      \"CoarseSaltAndPepper\": False,\n",
        "      \"JpegCompression\": False,\n",
        "      \"BlendAlpha\": False,\n",
        "      \"BlendAlphaRegularGrid\": False,\n",
        "      \"GaussianBlur\": False,\n",
        "      \"MotionBlur\": False,\n",
        "      \"MultiplyHueAndSaturation\": False,\n",
        "      \"Grayscale\": True,\n",
        "      \"ChangeColorTemperature\": True,\n",
        "      \"SigmoidContrast\": True,\n",
        "      \"CLAHE\": True,\n",
        "      \"Emboss\": True,\n",
        "      \"DirectedEdgeDetect\": True,\n",
        "      \"Fliplr\": True,\n",
        "      \"PiecewiseAffine\": True,\n",
        "      \"PerspectiveTransform\": True,\n",
        "      \"WithPolarWarping\": True,\n",
        "      \"Rot90\": True,\n",
        "      \"ElasticTransformation\": True,\n",
        "      \"Jigsaw\": True\n",
        "    }\n",
        "  }\n",
        "  emoji_hunt_object.update_config(cfg)\n",
        "\n",
        "def make_cfg_hard():\n",
        "  cfg = {\n",
        "    \"emoji_transforms\": {\n",
        "      \"Add\": True,\n",
        "      \"Multiply\": True,\n",
        "      \"Cutout\": True,\n",
        "      \"CoarseDropout\": True,\n",
        "      \"CoarseSaltAndPepper\": True,\n",
        "      \"JpegCompression\": True,\n",
        "      \"BlendAlpha\": True,\n",
        "      \"BlendAlphaRegularGrid\": True,\n",
        "      \"GaussianBlur\": True,\n",
        "      \"MotionBlur\": True,\n",
        "      \"MultiplyHueAndSaturation\": True,\n",
        "      \"Grayscale\": True,\n",
        "      \"ChangeColorTemperature\": True,\n",
        "      \"SigmoidContrast\": True,\n",
        "      \"CLAHE\": True,\n",
        "      \"Emboss\": True,\n",
        "      \"DirectedEdgeDetect\": True,\n",
        "      \"Fliplr\": True,\n",
        "      \"PiecewiseAffine\": True,\n",
        "      \"PerspectiveTransform\": True,\n",
        "      \"WithPolarWarping\": True,\n",
        "      \"Rot90\": True,\n",
        "      \"ElasticTransformation\": True,\n",
        "      \"Jigsaw\": True\n",
        "    }\n",
        "  }\n",
        "  emoji_hunt_object.update_config(cfg)\n",
        "\n",
        "def new_cfg():\n",
        "  cfg = {\n",
        "    \"emoji_transforms\": {\n",
        "      \"Add\": False,\n",
        "      \"Multiply\": False,\n",
        "      \"Cutout\": True,\n",
        "      \"CoarseDropout\": False,\n",
        "      \"CoarseSaltAndPepper\": True,\n",
        "      \"JpegCompression\": False,\n",
        "      \"BlendAlpha\": False,\n",
        "      \"BlendAlphaRegularGrid\": True,\n",
        "      \"GaussianBlur\": False,\n",
        "      \"MotionBlur\": False,\n",
        "      \"MultiplyHueAndSaturation\": False,\n",
        "      \"Grayscale\": False,\n",
        "      \"ChangeColorTemperature\": False,\n",
        "      \"SigmoidContrast\": False,\n",
        "      \"CLAHE\": False,\n",
        "      \"Emboss\": False,\n",
        "      \"DirectedEdgeDetect\": False,\n",
        "      \"Fliplr\": False,\n",
        "      \"PiecewiseAffine\": False,\n",
        "      \"PerspectiveTransform\": False,\n",
        "      \"WithPolarWarping\": False,\n",
        "      \"Rot90\": False,\n",
        "      \"ElasticTransformation\": True,\n",
        "      \"Jigsaw\": True\n",
        "    }\n",
        "  }\n",
        "  emoji_hunt_object.update_config(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "l9IEkyLbYJEJ"
      },
      "outputs": [],
      "source": [
        "#@title List comparison\n",
        "\n",
        "# Given a list of the results from the FAST and Harris tests, see if any points \n",
        "# overlap. If so, add them to the list of returned points\n",
        "# this is basically the same function as the neighbourhood points checker\n",
        "\n",
        "def point_matcher(L1, L2):\n",
        "\n",
        "  result = []\n",
        "  for L1_pt in L1:\n",
        "    for L2_pt in L2:\n",
        "      dx = L1_pt[0] - L2_pt[0]\n",
        "      dy = L1_pt[1] - L2_pt[1]\n",
        "      dist = np.sqrt(dx**2 + dy**2)\n",
        "      if dist < 100:\n",
        "        new_x = L1_pt[0] - dx\n",
        "        new_y = L1_pt[1] - dy\n",
        "        # return middlepoint between the two\n",
        "        result.append((new_x, new_y))\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "66sVEljRNEp0"
      },
      "outputs": [],
      "source": [
        "#@title FAST test code\n",
        "\n",
        "import cv2 \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def FAST_test(test_image, emoji_target, DEBUG = False):\n",
        "\n",
        "  # Convert both images to grayscale\n",
        "  gray_emoji = cv2.cvtColor(emoji_target, cv2.COLOR_BGR2GRAY)\n",
        "  gray_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # Create a FastFeatureDetector object\n",
        "  fast = cv2.FastFeatureDetector_create(threshold=25, nonmaxSuppression=True)\n",
        "\n",
        "  # Detect keypoints in the emoji and input images\n",
        "  emoji_kp = fast.detect(gray_emoji, None)\n",
        "  input_kp = fast.detect(gray_image, None)\n",
        "\n",
        "  # Create a SIFT object\n",
        "  sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "  # Compute descriptors for the keypoints in the emoji and input images\n",
        "  emoji_kp, emoji_desc = sift.compute(gray_emoji, emoji_kp)\n",
        "  input_kp, input_desc = sift.compute(gray_image, input_kp)\n",
        "\n",
        "  # Create a BFMatcher object\n",
        "  bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
        "\n",
        "  # Match the descriptors in the emoji and input images\n",
        "  matches = bf.match(input_desc, emoji_desc)\n",
        "  good_matches = sorted(matches, key = lambda x:x.distance)\n",
        "\n",
        "  image_kp = []\n",
        "  if len(matches) > 100:\n",
        "    lim = 100\n",
        "  else:\n",
        "    lim = len(matches)\n",
        "  for i in range(lim):\n",
        "    ind = matches[i].queryIdx\n",
        "    kp = input_kp[ind]\n",
        "    image_kp.append(kp)\n",
        "\n",
        "  # if its in a cluster of keypoints, just send one.\n",
        "  # Need at least some nearby to be confident its an emoji\n",
        "  result = []\n",
        "  required_neighbours = 1\n",
        "  sent = np.zeros(len(image_kp))\n",
        "  for iter, kps in enumerate(image_kp):\n",
        "    nearby_pts = 0\n",
        "    first_kp_xy = kps.pt #contains (x,y) - unused\n",
        "    for j in range(iter+1, len(image_kp)): # Search forward\n",
        "        sec_kp_xy = image_kp[j].pt # Contains (x,y) of second point\n",
        "        distance = np.sqrt((first_kp_xy[0]-sec_kp_xy[0])**2 + (first_kp_xy[1]-sec_kp_xy[1])**2)\n",
        "        if distance < 50: #threshold distance\n",
        "          nearby_pts += 1\n",
        "          sent[j] = 1\n",
        "    if nearby_pts > required_neighbours:\n",
        "      # print('appended ', kps.pt)\n",
        "      if sent[iter] != 1:\n",
        "        result.append(kps.pt) # appends (x,y) coordinate  \n",
        "\n",
        "  if DEBUG:\n",
        "    result_img = cv2.drawKeypoints(test_image, image_kp, 0, (255,0,0), 4)\n",
        "    result_emoji = cv2.drawKeypoints(emoji_target, emoji_kp, 0, (255,0,0), 4)\n",
        "\n",
        "    plt.figure(figsize = (10,10))\n",
        "    imgplot = plt.imshow(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB))\n",
        "    plt.show()\n",
        "    plt.figure(figsize = (3,3))\n",
        "    imgplot2 = plt.imshow(cv2.cvtColor(result_emoji, cv2.COLOR_BGR2RGB))\n",
        "    plt.show()\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ecacjzTrNZzC"
      },
      "outputs": [],
      "source": [
        "#@title Run FAST test\n",
        "\n",
        "# emoji_hunt_object = EmojiHunt() \n",
        "# make_cfg_easy()\n",
        "make_cfg_med()\n",
        "# make_cfg_med2()\n",
        "# make_cfg_hard()\n",
        "test_image, emoji_target, gt_points = emoji_hunt_object.generate_image_and_points()\n",
        "score = emoji_hunt_object.score_function(gt_points, FAST_test(test_image, emoji_target))\n",
        "print('score is: ', score)\n",
        "FAST_test(test_image, emoji_target, True)\n",
        "# emoji_hunt_object.offical_test(SIFT_test,emoji_hunt_object.get_config())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "L_fzFGoWKDNC"
      },
      "outputs": [],
      "source": [
        "#@title Harris-Stephens Corner Detector Code\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def Harris_test(emoji, image, DEBUG = False):\n",
        "  # emoji_hunt_object = EmojiHunt()\n",
        "  # test_image, emoji_target, gt_points = emoji_hunt_object.generate_image_and_points()\n",
        "\n",
        "  # Convert both images to grayscale\n",
        "  gray_emoji = cv2.cvtColor(emoji_target, cv2.COLOR_BGR2GRAY)\n",
        "  gray_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # Create a Harris-Stevens corner detector object\n",
        "  harris_emoji = cv2.cornerHarris(gray_emoji, 3, 13, 0.04)\n",
        "  harris_image = cv2.cornerHarris(gray_image, 3, 13, 0.04)\n",
        "\n",
        "  # Normalize the detector response\n",
        "  cv2.normalize(harris_emoji, harris_emoji, 0, 255, cv2.NORM_MINMAX)\n",
        "  cv2.normalize(harris_image, harris_image, 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "  # Threshold the response to get keypoints\n",
        "  emoji_kp = []\n",
        "  threshold_emoji = 0.1 * harris_emoji.max()\n",
        "  for i in range(harris_emoji.shape[0]):\n",
        "      for j in range(harris_emoji.shape[1]):\n",
        "          if harris_emoji[i,j] > threshold_emoji:\n",
        "              emoji_kp.append(cv2.KeyPoint(j, i, 3))\n",
        "\n",
        "  image_kp = []\n",
        "  threshold_image = 0.1 * harris_image.max()\n",
        "  for i in range(harris_image.shape[0]):\n",
        "      for j in range(harris_image.shape[1]):\n",
        "          if harris_image[i,j] > threshold_image:\n",
        "              image_kp.append(cv2.KeyPoint(j, i, 3))\n",
        "\n",
        "  # Create a SIFT object\n",
        "  sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "  # Compute descriptors for the keypoints in the emoji and input images\n",
        "  emoji_kp, emoji_desc = sift.compute(emoji_target, emoji_kp)\n",
        "  image_kp, image_desc = sift.compute(test_image, image_kp)\n",
        "\n",
        "  # Create a BFMatcher object\n",
        "  bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
        "\n",
        "  # Match the descriptors in the emoji and input images\n",
        "  matches = bf.match(image_desc, emoji_desc)\n",
        "  good_matches = sorted(matches, key = lambda x:x.distance)\n",
        "\n",
        "  # Also could use KNN Matcher\n",
        "  # matcher = cv2.DescriptorMatcher_create(cv2.DescriptorMatcher_FLANNBASED)\n",
        "  # matches = matcher.knnMatch(image_desc, emoji_desc, 2)\n",
        "  # matches = []\n",
        "  # for m,n in matches:\n",
        "  #   if m.distance < 0.7*n.distance:\n",
        "  #     matches.append(m)\n",
        "\n",
        "  the_kp = []\n",
        "  if len(matches) > 100:\n",
        "    lim = 100\n",
        "  else:\n",
        "    lim = len(matches)\n",
        "  for i in range(lim):\n",
        "      kp = image_kp[matches[i].queryIdx]\n",
        "      the_kp.append(kp)\n",
        "\n",
        "  # Turn keypoints into xy points to return back\n",
        "  result = []\n",
        "  required_neighbours = 1\n",
        "  sent = np.zeros(len(the_kp))\n",
        "  for iter, kps in enumerate(the_kp):\n",
        "    nearby_pts = 0\n",
        "    first_kp_xy = kps.pt #contains (x,y)\n",
        "    for j in range(iter+1, len(the_kp)): # Search forward only\n",
        "        sec_kp_xy = the_kp[j].pt # Contains (x,y) of second point\n",
        "        distance = np.sqrt((first_kp_xy[0]-sec_kp_xy[0])**2 + (first_kp_xy[1]-sec_kp_xy[1])**2)\n",
        "        if distance < 50: #threshold distance, about half the length of an emoji\n",
        "          nearby_pts += 1\n",
        "          sent[j] = 1 # dont send back a point if it is already been marked as a neighbour point\n",
        "    if nearby_pts > required_neighbours:\n",
        "      # print('appended ', kps.pt)\n",
        "      if sent[iter] != 1:\n",
        "        result.append(kps.pt) # appends (x,y) coordinate\n",
        "\n",
        "  if DEBUG:\n",
        "    result_img = cv2.drawKeypoints(test_image, the_kp, 0, (255,0,0), 4)\n",
        "    result_emoji = cv2.drawKeypoints(emoji_target, the_kp, 0, (255,0,0), 4)\n",
        "\n",
        "    plt.figure(figsize = (10,10))\n",
        "    imgplot = plt.imshow(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB))\n",
        "    plt.show()\n",
        "    plt.figure(figsize = (3,3))\n",
        "    imgplot2 = plt.imshow(cv2.cvtColor(result_emoji, cv2.COLOR_BGR2RGB))\n",
        "    plt.show()\n",
        "\n",
        "  return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "THkmO2qZbTMH"
      },
      "outputs": [],
      "source": [
        "#@title Run Harris-Stephens test\n",
        "\n",
        "# emoji_hunt_object = EmojiHunt() \n",
        "make_cfg_easy()\n",
        "# make_cfg_med()\n",
        "# make_cfg_med2()\n",
        "# make_cfg_hard()\n",
        "test_image, emoji_target, gt_points = emoji_hunt_object.generate_image_and_points()\n",
        "score = emoji_hunt_object.score_function(gt_points, Harris_test(test_image, emoji_target, False))\n",
        "print('score is: ', score)\n",
        "Harris_test(test_image, emoji_target, True)\n",
        "# emoji_hunt_object.offical_test(Harris_test,emoji_hunt_object.get_config())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQpXBaiatB7A",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title SIFT test Code\n",
        "import cv2 \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def SIFT_test(image, emoji, DEBUG = False):\n",
        "  img_w_kps = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  emoji_with_keypoints = emoji #cv2.cvtColor(emoji, cv2.COLOR_BGR2RGB)\n",
        "  emoji = cv2.cvtColor(emoji, cv2.COLOR_BGR2GRAY)\n",
        "  BF_MATCHER = True\n",
        "\n",
        "  # sift = cv2.xfeatures2d.SIFT_create()\n",
        "  sift = cv2.xfeatures2d.SIFT_create(0,15, 0.04, 10, 1.6) # (max features, octave layers (this is levels per octave))\n",
        "  #(int nfeatures=0, int nOctaveLayers=3, double contrastThreshold=0.04, double edgeThreshold=10, double sigma=1.6)\n",
        "  # Can increase levels per octave to return more refined keypoints.\n",
        "  # Raising levels per octave too much will cause issues due to noise, see vlfeat link\n",
        "  # https://www.vlfeat.org/api/sift.html\n",
        "  # https://docs.opencv.org/3.4/d7/d60/classcv_1_1SIFT.html\n",
        "\n",
        "  keypoints_1, descriptors_1 = sift.detectAndCompute(img,None)\n",
        "  keypoints_2, descriptors_2 = sift.detectAndCompute(emoji,None)\n",
        "  # .pt returns (x,y) point of keypoint\n",
        "  # https://docs.opencv.org/4.x/d2/d29/classcv_1_1KeyPoint.html\n",
        "\n",
        "  #brute force feature matching\n",
        "  if BF_MATCHER:\n",
        "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
        "    # #set crossCheck to false to return all possible matches\n",
        "    good_matches = bf.match(descriptors_1,descriptors_2)\n",
        "    good_matches = sorted(good_matches, key = lambda x:x.distance)\n",
        "    if DEBUG:\n",
        "      print('good_matches shape is ', np.shape(good_matches))\n",
        "      print('matches type is ', type(good_matches))\n",
        "    # Sorting BF matches gives best matches first and by keeping only a select\n",
        "    # number of best matches, we cut down time by not sifting through all matches.\n",
        "\n",
        "  # knn matches is a tuple of tuples of cv2.DMatch\n",
        "  # it is 3123 x 2 sets of DMatch. features x num of nearest neighbours\n",
        "  # DMatch holds query_index, train index and a float distance\n",
        "  # accessible with .queryIdx, .imgIdx, .distance\n",
        "  # https://docs.opencv.org/3.4/d4/de0/classcv_1_1DMatch.html\n",
        "\n",
        "  # Lowering this threshold removes false positives from img, but taking it \n",
        "  # too far prevents any emoji detection. \n",
        "  # 0.7 gives some false positives, 0.5 is a good balance\n",
        "  else: # If not BF matcher -> KNN matcher\n",
        "    matcher = cv2.DescriptorMatcher_create(cv2.DescriptorMatcher_FLANNBASED)\n",
        "    matches = matcher.knnMatch(descriptors_1, descriptors_2, 2)\n",
        "    good_matches = []\n",
        "    for m,n in matches:\n",
        "      if m.distance < 0.7*n.distance:\n",
        "        good_matches.append(m)\n",
        "    if DEBUG:\n",
        "      print('descriptors shape is ', np.shape(descriptors_2))\n",
        "      print('matches shape is ', np.shape(matches))\n",
        "      print('matches type is ', type(matches))\n",
        "\n",
        "  if BF_MATCHER:\n",
        "    upper_limit = 500 # keep first 500 keypoints from BF matcher. \n",
        "    # reduces execution time without throwing out good matches since theyre ordered \n",
        "  else: \n",
        "    upper_limit = len(good_matches)\n",
        "    # Keep all matches for KNN matching\n",
        "  \n",
        "  # turn matches into keypoints for plotting\n",
        "  best_kps = []\n",
        "  for i in range(upper_limit):\n",
        "    query_idx = good_matches[i].queryIdx\n",
        "    point = keypoints_1[query_idx].pt # get well-matched keypoint from the img\n",
        "    x = point[0]\n",
        "    y = point[1]\n",
        "    kp = cv2.KeyPoint(x,y,5)\n",
        "    best_kps.append(kp)\n",
        "  best_kps_set = [*set(best_kps)] # set to remove duplicates\n",
        "  best_kps = list(best_kps_set)   # list to keep compatibility\n",
        "  if DEBUG:\n",
        "    print('best_kps len is ',len(best_kps))\n",
        "    print('emoji_kps len is ',len(keypoints_2))\n",
        "  # Keypoints are required for the drawkeypoints function \n",
        "\n",
        "  # if its in a cluster of keypoints, just send one.\n",
        "  # Need at least some nearby to be confident its an emoji\n",
        "  result = []\n",
        "  required_neighbours = 2\n",
        "  sent = np.zeros(len(best_kps))\n",
        "  for iter, kps in enumerate(best_kps):\n",
        "    nearby_pts = 0\n",
        "    max_neighbours = 0\n",
        "    first_kp_xy = kps.pt #contains (x,y)\n",
        "    for j in range(iter+1, len(best_kps)): # Search forward\n",
        "      # if i != j:\n",
        "        sec_kp_xy = best_kps[j].pt # Contains (x,y) of second point\n",
        "        distance = np.sqrt((first_kp_xy[0]-sec_kp_xy[0])**2 + (first_kp_xy[1]-sec_kp_xy[1])**2)\n",
        "        if distance < 100: #threshold distance - one width of an emoji\n",
        "          nearby_pts += 1\n",
        "          sent[j] = 1\n",
        "    if nearby_pts > required_neighbours:\n",
        "      # print('appended ', kps.pt)\n",
        "      if sent[iter] != 1:\n",
        "        result.append(kps.pt) # appends (x,y) coordinate\n",
        "  \n",
        "  if (len(result)) < 2:\n",
        "    result.append((256, 256)) # Dont send back nearly zero guesses. At least gamble\n",
        "  if (len(result)) > 8:\n",
        "    result = result [:8]  # typically, images did not include more than 8 emojis\n",
        "\n",
        "  res_kps = []\n",
        "  for i in range(len(result)):\n",
        "    x = result[i][0]\n",
        "    y = result[i][1]\n",
        "    kp = cv2.KeyPoint(x,y,5)\n",
        "    res_kps.append(kp)\n",
        "\n",
        "  if DEBUG:\n",
        "    print('sending back ', len(result), 'points.')\n",
        "    for pt in result:\n",
        "      print(pt)\n",
        "\n",
        "  if DEBUG:\n",
        "    #Plot images\n",
        "    # img_w_kps = cv2.drawKeypoints(img_w_kps, best_kps, 0, (255,64,255), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "    img_w_kps = cv2.drawKeypoints(img_w_kps, res_kps, 0, (255,64,255), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "    plt.figure(figsize = (10,10))\n",
        "    plt.imshow(img_w_kps),plt.show()\n",
        "\n",
        "    emoji_with_keypoints = cv2.drawKeypoints(emoji_with_keypoints, keypoints_2, 0, (255,0,0), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "    plt.imshow(cv2.cvtColor(emoji_with_keypoints, cv2.COLOR_BGR2RGB)),plt.show()\n",
        "    # plt.imshow(cv2.cvtColor(emoji_target, cv2.COLOR_BGR2RGB)\n",
        "  \n",
        "  return result\n",
        "\n",
        "# SIFT_test(test_image, emoji_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0wOYYjbz9lG",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Run SIFT test\n",
        "\n",
        "# emoji_hunt_object = EmojiHunt() \n",
        "make_cfg_easy()\n",
        "# make_cfg_med()\n",
        "# make_cfg_med2()\n",
        "# make_cfg_hard()\n",
        "new_cfg()\n",
        "test_image, emoji_target, gt_points = emoji_hunt_object.generate_image_and_points()\n",
        "score = emoji_hunt_object.score_function(gt_points, SIFT_test(test_image, emoji_target))\n",
        "print('score is: ', score)\n",
        "SIFT_test(test_image, emoji_target, True)\n",
        "# emoji_hunt_object.offical_test(SIFT_test,emoji_hunt_object.get_config())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "C8ik_SWaipRC"
      },
      "outputs": [],
      "source": [
        "#@title Triple Test\n",
        "\n",
        "# This is a combination of all 3 methods. Since SIFT already had great accuracy, \n",
        "# we wanted to check if it missed any points (and not take any points away).\n",
        "# We checked if FAST and Harris tests were able to detects emojis in the same neighbourhood\n",
        "# if so, they were concatenated onto the list returned by SIFT.\n",
        "# This test was not used since it took far to long to execute. Sift took about 3s to run\n",
        "# this one took 35 seconds to run one iteration. A full official test took 30 minutes and \n",
        "# scored worse than just SIFT, so we opted for only using SIFT.\n",
        "\n",
        "# emoji_hunt_object = EmojiHunt() \n",
        "make_cfg_easy()\n",
        "# make_cfg_med()\n",
        "# make_cfg_med2()\n",
        "# make_cfg_hard()\n",
        "test_image, emoji_target, gt_points = emoji_hunt_object.generate_image_and_points()\n",
        "\n",
        "def triple_test(test_image, emoji_target):\n",
        "  HS_FAST_list = point_matcher(FAST_test(test_image, emoji_target), Harris_test(test_image, emoji_target))\n",
        "  SIFT_list = SIFT_test(test_image, emoji_target)\n",
        "  result_list = HS_FAST_list + SIFT_list\n",
        "  return result_list\n",
        "\n",
        "triple_test_list = triple_test(test_image, emoji_target)\n",
        "score = emoji_hunt_object.score_function(gt_points, triple_test_list)\n",
        "print('score is :', score)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjw8WVHfWRud",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Update Config & Run Official Test\n",
        "emoji_hunt_object = EmojiHunt()\n",
        "test_image, emoji_target, gt_points = emoji_hunt_object.generate_image_and_points()\n",
        "#print(json.dumps(emoji_hunt_object.get_config(), indent=2))\n",
        "\n",
        "# make_cfg_easy()\n",
        "# make_cfg_med()\n",
        "# make_cfg_med2()\n",
        "# make_cfg_hard()\n",
        "new_cfg()\n",
        "\n",
        "emoji_hunt_object.offical_test(SIFT_test,emoji_hunt_object.get_config())\n",
        "# emoji_hunt_object.offical_test(triple_test,emoji_hunt_object.get_config())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDNbQGk0skMi",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Generate and Plot new test case\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "test_image, emoji_target, gt_points = emoji_hunt_object.generate_image_and_points()\n",
        "\n",
        "plt.figure(figsize = (10,10))\n",
        "imgplot = plt.imshow(cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "plt.figure(figsize = (3,3))\n",
        "imgplot2 = plt.imshow(cv2.cvtColor(emoji_target, cv2.COLOR_BGR2RGB))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "25EfNXKp09KK"
      },
      "outputs": [],
      "source": [
        "#@title EmojiHunt Code\n",
        "from PIL import Image, ImageFont\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from keras.datasets import cifar100\n",
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "import time\n",
        "import pkgutil\n",
        "import urllib.request\n",
        "import zlib\n",
        "import json\n",
        "\n",
        "class EmojiHunt():\n",
        "    def __init__(self):\n",
        "        self.config = {\n",
        "        'emoji_transforms':\n",
        "        {\n",
        "            \"Add\" : True,\n",
        "            \"Multiply\" : True,\n",
        "            \"Cutout\" : True,\n",
        "            \"CoarseDropout\" : True,\n",
        "            \"CoarseSaltAndPepper\" : True,\n",
        "            \"JpegCompression\" : True,\n",
        "            \"BlendAlpha\" : True,\n",
        "            \"BlendAlphaRegularGrid\" : True,\n",
        "            \"GaussianBlur\" : True,\n",
        "            \"MotionBlur\" : True,\n",
        "            \"MultiplyHueAndSaturation\" : True,\n",
        "            \"Grayscale\" : True,\n",
        "            \"ChangeColorTemperature\" : True,\n",
        "            \"SigmoidContrast\" : True,\n",
        "            \"CLAHE\" : True,\n",
        "            \"Emboss\" : True,\n",
        "            \"DirectedEdgeDetect\" : True,\n",
        "            \"Fliplr\" : True,\n",
        "            \"PiecewiseAffine\" : True,\n",
        "            \"PerspectiveTransform\" : True,\n",
        "            \"WithPolarWarping\" : True,\n",
        "            \"Rot90\" : True,\n",
        "            \"ElasticTransformation\" : True,\n",
        "            \"Jigsaw\" : True,\n",
        "        }\n",
        "    }\n",
        "        self.update_config(self.config)\n",
        "        self.emoji_size = 48\n",
        "        print('Downloading assets...')\n",
        "        urllib.request.urlretrieve(\"https://github.com/dash-uvic/ece471_536-S2022/raw/main/duck-hunt/NotoSansSC-Regular.otf\", \"NotoSansSC-Regular.otf\")\n",
        "        urllib.request.urlretrieve(\"https://github.com/dash-uvic/ece471_536-S2022/raw/main/duck-hunt/emojis.npy\", \"emojis.npy\")\n",
        "        self.emojies_master = np.load('emojis.npy')\n",
        "        (_, __), (imgs, ___) = cifar100.load_data()\n",
        "        self.background_parts = imgs.copy()\n",
        "        self.background_parts[:,:,:,0] = imgs[:,:,:,2] \n",
        "        self.background_parts[:,:,:,2] = imgs[:,:,:,0]\n",
        "\n",
        "    def get_config(self):\n",
        "        return self.config\n",
        "\n",
        "    def update_config(self, config):\n",
        "        self.config = config\n",
        "        self.emoji_augs = []\n",
        "        # Emoji Augs\n",
        "        if self.config['emoji_transforms']['Add']:\n",
        "            self.emoji_augs.append(iaa.Add((-40, 40), per_channel=0.5))\n",
        "        if self.config['emoji_transforms']['Multiply']:\n",
        "            self.emoji_augs.append(iaa.Multiply((0.5, 1.5), per_channel=0.5))\n",
        "        if self.config['emoji_transforms']['Cutout']:\n",
        "            self.emoji_augs.append(iaa.Cutout(nb_iterations=2))\n",
        "        if self.config['emoji_transforms']['CoarseDropout']:\n",
        "            self.emoji_augs.append(iaa.CoarseDropout((0.0, 0.05), size_percent=(0.02, 0.25)))\n",
        "        if self.config['emoji_transforms']['CoarseSaltAndPepper']:\n",
        "            self.emoji_augs.append(iaa.CoarseSaltAndPepper(0.05, size_px=(4, 16)))\n",
        "        if self.config['emoji_transforms']['JpegCompression']:\n",
        "            self.emoji_augs.append(iaa.JpegCompression(compression=(70, 99)))\n",
        "        if self.config['emoji_transforms']['BlendAlpha']:\n",
        "            self.emoji_augs.append(iaa.BlendAlpha((0.0, 1.0), iaa.Grayscale(1.0)))\n",
        "        if self.config['emoji_transforms']['BlendAlphaRegularGrid']:\n",
        "            self.emoji_augs.append(iaa.BlendAlphaRegularGrid(nb_rows=(1, 8), nb_cols=(1, 8),foreground=iaa.Multiply(0.0)))\n",
        "        if self.config['emoji_transforms']['GaussianBlur']:\n",
        "            self.emoji_augs.append(iaa.GaussianBlur(sigma=(0.0, 1.0)))\n",
        "        if self.config['emoji_transforms']['MotionBlur']:\n",
        "            self.emoji_augs.append(iaa.MotionBlur(k=5, angle=[-45, 45]))\n",
        "        if self.config['emoji_transforms']['MultiplyHueAndSaturation']:\n",
        "            self.emoji_augs.append(iaa.MultiplyHueAndSaturation(mul_hue=(0.75, 1.25)))\n",
        "        if self.config['emoji_transforms']['Grayscale']:\n",
        "            self.emoji_augs.append(iaa.Grayscale(alpha=(0.0, 1.0)))\n",
        "        if self.config['emoji_transforms']['ChangeColorTemperature']:\n",
        "            self.emoji_augs.append(iaa.ChangeColorTemperature((1100, 10000)))\n",
        "        if self.config['emoji_transforms']['SigmoidContrast']:\n",
        "            self.emoji_augs.append(iaa.SigmoidContrast(gain=(3, 10), cutoff=(0.4, 0.6), per_channel=True))\n",
        "        if self.config['emoji_transforms']['CLAHE']:\n",
        "            self.emoji_augs.append(iaa.CLAHE(clip_limit=(1, 5)))\n",
        "        if self.config['emoji_transforms']['Emboss']:\n",
        "            self.emoji_augs.append(iaa.Emboss(alpha=(0.0, 0.5), strength=(0.5, 1.5)))\n",
        "        if self.config['emoji_transforms']['DirectedEdgeDetect']:\n",
        "            self.emoji_augs.append(iaa.DirectedEdgeDetect(alpha=(0.0, 0.5), direction=(0.0, 1.0)))\n",
        "        if self.config['emoji_transforms']['Fliplr']:\n",
        "            self.emoji_augs.append(iaa.Fliplr(0.5))\n",
        "        if self.config['emoji_transforms']['PiecewiseAffine']:\n",
        "            self.emoji_augs.append(iaa.PiecewiseAffine(scale=(0.03, 0.15)))\n",
        "        if self.config['emoji_transforms']['PerspectiveTransform']:\n",
        "            self.emoji_augs.append(iaa.PerspectiveTransform(scale=(0.01, 0.25)))\n",
        "        if self.config['emoji_transforms']['WithPolarWarping']:\n",
        "            self.emoji_augs.append(iaa.WithPolarWarping(iaa.CropAndPad(percent=(-0.15, 0.15))))\n",
        "        if self.config['emoji_transforms']['Rot90']:\n",
        "            self.emoji_augs.append(iaa.Rot90([1, 3]))\n",
        "        if self.config['emoji_transforms']['ElasticTransformation']:\n",
        "            self.emoji_augs.append(iaa.ElasticTransformation(alpha=(0, 2.0), sigma=0.15))\n",
        "        if self.config['emoji_transforms']['Jigsaw']:\n",
        "            self.emoji_augs.append(iaa.Jigsaw(nb_rows=4, nb_cols=4))\n",
        "\n",
        "    def get_random_emoji_img(self):\n",
        "            \n",
        "        return cv2.resize(self.emojies_master[random.randint(0,self.emojies_master.shape[0]-1)],(self.emoji_size,self.emoji_size))\n",
        "\n",
        "    def get_background(self):\n",
        "        background = np.zeros((512,512,3), np.uint8)\n",
        "        for x in range(512//self.emoji_size):\n",
        "            for y in range(512//self.emoji_size):\n",
        "                background[x*self.emoji_size:(x+1)*self.emoji_size,y*self.emoji_size:(y+1)*self.emoji_size] = cv2.resize(\n",
        "                                        self.background_parts[random.randint(0,self.background_parts.shape[0]-1)], (self.emoji_size,self.emoji_size))\n",
        "        return background\n",
        "\n",
        "    def augment_emoji(self, emoji):\n",
        "        transformation_aug = iaa.Sequential([iaa.Rotate((-180, 180)),iaa.SomeOf((0, 5), self.emoji_augs)],random_order=True)\n",
        "        images_aug = transformation_aug(image=emoji)\n",
        "        return images_aug\n",
        "\n",
        "    def generate_image_and_points(self):\n",
        "        '''\n",
        "        Returns a target image, emoji example (uncorrupted), and ground truth gt_points. For your testing and development\n",
        "        '''\n",
        "        emoji_target = self.get_random_emoji_img()\n",
        "        test_image   = self.get_background()\n",
        "\n",
        "        gt_points = []\n",
        "\n",
        "        for _ in range(random.randint(1,10)):\n",
        "            x = random.randint(0,512-self.emoji_size-1)\n",
        "            y = random.randint(0,512-self.emoji_size-1)\n",
        "            gt_points.append((x+self.emoji_size//2,y+self.emoji_size//2))\n",
        "            augmented_emoji = self.augment_emoji(emoji_target)\n",
        "            test_image[x:x+self.emoji_size,y:y+self.emoji_size] = np.where(\n",
        "                np.expand_dims(np.sum(augmented_emoji,axis=-1) > 25,-1), augmented_emoji, test_image[x:x+self.emoji_size,y:y+self.emoji_size])\n",
        "        \n",
        "        test_image_agus = [iaa.AdditiveGaussianNoise(scale=(0, 0.1*255)),\n",
        "                            iaa.GaussianBlur(sigma=(0.0, 0.25))]\n",
        "\n",
        "        test_image = iaa.Sequential(test_image_agus)(image=test_image)\n",
        "\n",
        "        return test_image, emoji_target, gt_points\n",
        "\n",
        "    def score_function(self, ground_truth, predicted):\n",
        "        '''\n",
        "        Pass this function your predictions and ground truth to get your scores. \n",
        "        '''\n",
        "        distances = np.zeros((len(ground_truth),len(predicted)))\n",
        "        for x in range(len(ground_truth)):\n",
        "            for y in range(len(predicted)):\n",
        "                distances[x,y] = (ground_truth[x][0]-predicted[y][0])**2+(ground_truth[x][1]-predicted[y][1])**2\n",
        "\n",
        "        distances = np.sqrt(distances)\n",
        "        scores = []\n",
        "        for x in range(len(predicted)):\n",
        "            min_index = np.argmin(distances)\n",
        "            scores.append(np.min(distances))\n",
        "            distances[min_index//len(predicted),:] = 1e10\n",
        "            if np.mean(distances) == 1e10:\n",
        "                break\n",
        "\n",
        "        while len(scores) < max([len(ground_truth),len(predicted)]):\n",
        "            scores.append(512)\n",
        "\n",
        "        return np.mean(scores)\n",
        "        \n",
        "    def offical_test(self, function, config):\n",
        "        '''\n",
        "        Runs an official test of 10 runs of your function! \n",
        "        Pass a function which takes in the inputs [image, emoji] and returns a lists of poits [(x,y),(x_2,y_2)] for the guesses of each emoji location.\n",
        "        This function will use a randomized seed every time not your pre-set seed used for debugging.\n",
        "        '''\n",
        "        seed_val = sum([int(byte) for byte in zlib.compress(json.dumps(config).encode())])\n",
        "        random.seed(seed_val)\n",
        "        ia.seed(seed_val)\n",
        "        \n",
        "        self.update_config(config)\n",
        "        scores = []\n",
        "        for x in range(50):\n",
        "            test_image, emoji_target, gt_points = self.generate_image_and_points()\n",
        "            scores.append(self.score_function(gt_points, function(test_image, emoji_target)))\n",
        "        \n",
        "        print(\"~~~~~STARTING TEST~~~~~~\")\n",
        "        print(\"Enabled Augmentations....\")\n",
        "        for aug in config['emoji_transforms'].keys():\n",
        "            if config['emoji_transforms'][aug]:\n",
        "                print('    ', aug)\n",
        "        print(\"Scores (lower is better)....\")\n",
        "        for x, score in enumerate(scores):\n",
        "            print('    Run',x,'->',score)\n",
        "\n",
        "        print('Final Score is:', np.mean(scores))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':                \n",
        "\n",
        "    config = {\n",
        "        'emoji_transforms':\n",
        "        {\n",
        "            \"Add\" : True,\n",
        "            \"Multiply\" : True,\n",
        "            \"Cutout\" : True,\n",
        "            \"CoarseDropout\" : True,\n",
        "            \"CoarseSaltAndPepper\" : True,\n",
        "            \"JpegCompression\" : True,\n",
        "            \"BlendAlpha\" : True,\n",
        "            \"BlendAlphaRegularGrid\" : True,\n",
        "            \"GaussianBlur\" : True,\n",
        "            \"MotionBlur\" : True,\n",
        "            \"MultiplyHueAndSaturation\" : True,\n",
        "            \"Grayscale\" : True,\n",
        "            \"ChangeColorTemperature\" : True,\n",
        "            \"SigmoidContrast\" : True,\n",
        "            \"CLAHE\" : True,\n",
        "            \"Emboss\" : True,\n",
        "            \"DirectedEdgeDetect\" : True,\n",
        "            \"Fliplr\" : True,\n",
        "            \"PiecewiseAffine\" : True,\n",
        "            \"PerspectiveTransform\" : True,\n",
        "            \"WithPolarWarping\" : True,\n",
        "            \"Rot90\" : True,\n",
        "            \"ElasticTransformation\" : True,\n",
        "            \"Jigsaw\" : True,\n",
        "        }\n",
        "    }\n",
        "\n",
        "    test = EmojiHunt()\n",
        "\n",
        "    a = [(0,1),(15,15), (600,600), (0,0)]\n",
        "    b = [(5,1),(15,20), (600,605)]\n",
        "    print(test.score_function(a,b))\n",
        "\n",
        "    def test_bad_function(image, emoji):\n",
        "        return [(random.randint(0,512),random.randint(0,512))]\n",
        "\n",
        "    test.offical_test(test_bad_function,config)\n",
        "\n",
        "    while True:\n",
        "        image, emoji, points = test.generate_image_and_points()\n",
        "        cv2.imshow('', image)\n",
        "        cv2.waitKey(-1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}